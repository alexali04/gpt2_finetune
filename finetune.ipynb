{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/alexali04/gpt2_finetune/blob/main/finetune.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2ca_6hhn1mwb"
      },
      "source": [
        "This Google Colab will detail how I fine-tuned GPT2 on movie reviews in order to improve its auto-regressive capacity in that domain."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## make sure to use T4 GPU\n",
        "\n",
        "!pip install transformers\n",
        "\n",
        "import os\n",
        "\n",
        "os.environ['CUDA_LAUNCH_BLOCKING'] = \"1\"\n",
        "\n",
        "import torch\n",
        "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
        "\n",
        "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
        "model = GPT2LMHeadModel.from_pretrained(\"gpt2\")"
      ],
      "metadata": {
        "id": "X0jzAGAqeTGj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2AVHaXFy51Qb"
      },
      "source": [
        "\n",
        "\n",
        "# Pre-Processing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BU6d8oev4qJ_"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import random\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from transformers import AdamW, get_linear_schedule_with_warmup\n",
        "from tqdm import tqdm, trange\n",
        "import torch.nn.functional as F\n",
        "import csv"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## much faster to load the CSVs from drive than into Colab directly - name them whatever you like but be sure the path / filenames match\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "summaries = pd.read_csv('/content/drive/MyDrive/summaries.csv')\n",
        "reviews = pd.read_csv('/content/drive/MyDrive/reviews.csv', engine='python', error_bad_lines=False)\n",
        "## ignore bad lines - customize as you like\n",
        "\n",
        "print(summaries.shape) ## gut check\n",
        "print(reviews.shape)"
      ],
      "metadata": {
        "id": "joGsN9rgd_b9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "summaries_relevant = summaries[['uid', 'synopsis']].rename(columns = {'uid':'summary_uid', 'synopsis':'text'})\n",
        "reviews_relevant = reviews[['uid', 'synopsis']].rename(columns = {'uid': 'anime_uid'})\n",
        "reviews_relevant = reviews[['uid', 'text']]\n",
        "reviews_relevant['text'] = reviews_relevant['text'].str.replace(\"more pics\", \"\", case=False, regex=True)\n",
        "reviews_relevant = reviews_relevant.sample(n = 18000) # number subject to change\n",
        "animes_relevant = summaries_relevant.sample(n = 18000)\n",
        "df = pd.concat([summaries_relevant, reviews_relevant], ignore_index=True)"
      ],
      "metadata": {
        "id": "Gack8YopeDdh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W_Ia3R_UQSmT",
        "outputId": "79a641a9-0a5b-4138-e345-71347cc9c675"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(35090, 2)"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ],
      "source": [
        "df = df[df['text'].notnull()]\n",
        "df = df[df['uid'].notnull()]\n",
        "df = df[df['text'].str.strip() != \"\"] ## filter out empty values\n",
        "print(df.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PEx4KtmxAZyw"
      },
      "outputs": [],
      "source": [
        "small_dataset = df.sample(2000) ## gonna use small data set as trial run\n",
        "df = df.sample(n = 24000) # number subject to change\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7NYQ9jtCiQOv",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "030df859-f777-4f09-b4ba-95c65b9a2b34"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(23995, 2)\n"
          ]
        }
      ],
      "source": [
        "test_set = df.sample(n = 5)\n",
        "df = df.loc[~df.index.isin(test_set.index)]\n",
        "print(df.shape)\n",
        "\n",
        "# reset indices\n",
        "test_set = test_set.reset_index()\n",
        "df = df.reset_index()\n",
        "\n",
        "# for the test set keep last 20 words in a new col and remove from original col\n",
        "test_set['true_end'] = test_set['text'].str.split().str[-20:].apply(' '.join)\n",
        "test_set['text'] = test_set['text'].str.split().str[:-20].apply(' '.join)\n",
        "#test_set.to_csv(\"/content/test.csv\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_set.head() ## gut check"
      ],
      "metadata": {
        "id": "dFu3SlWbeLu7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Reviews(Dataset):\n",
        "  def __init__(self, control_code, truncate = False, gpt2_type = \"gpt2\", max_length = 1024):\n",
        "\n",
        "    self.tokenizer = GPT2Tokenizer.from_pretrained(gpt2_type) ## tokenizer will reprsent 'subwords' as integers to pass into neural network\n",
        "    ## breaks up sentences into tokens so the model can interpret them / learn relations between them\n",
        "    ## subword tokenization > word / character tokenization - handles OOV well and isn't as long as character tokenization\n",
        "    self.reviews = []\n",
        "\n",
        "    for i, row in enumerate(df['text']):\n",
        "      tokens = self.tokenizer.encode(f\"<|{control_code}|>{row}\")\n",
        "      truncated_tokens = tokens[:max_length]\n",
        "      self.reviews.append(torch.tensor(truncated_tokens))\n",
        "      if i % 1000 == 0:\n",
        "        print(i)\n",
        "      if truncate and i == 20000:\n",
        "        break\n",
        "\n",
        "\n",
        "      ## what we're doing is converting each review in df['text'] into tokens, putting those in a tensor, and putting those tokens into the model\n",
        "\n",
        "   ## if truncate:\n",
        "    ##  self.reviews = self.reviews[:20000]\n",
        "\n",
        "    self.reviews_count = len(self.reviews)\n",
        "\n",
        "\n",
        "  def __len__(self):\n",
        "    return self.reviews_count\n",
        "\n",
        "  def __getitem__(self, item):\n",
        "    return self.reviews[item]\n",
        "\n",
        "#dataset = Reviews(df['text'], truncate = False, gpt2_type = \"gpt2\")\n",
        "dataset = Reviews(small_dataset['text'], truncate = False, gpt2_type = \"gpt2\")\n"
      ],
      "metadata": {
        "id": "f22_L94AeOQI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jys3F3RJvXjq"
      },
      "outputs": [],
      "source": [
        "## move in the direction anti-parallel to the average of the computed gradients of the loss function\n",
        "\n",
        "def pack_tensor(new_tensor, packed_tensor, max_seq_len):\n",
        "  if packed_tensor is None:\n",
        "    return new_tensor, True, None\n",
        "\n",
        "  if new_tensor.size()[1] + packed_tensor.size()[1] > max_seq_len:\n",
        "    return new_tensor, False, new_tensor\n",
        "  else:\n",
        "    packed_tensor = torch.cat([new_tensor, packed_tensor[:, 1:]], dim = 1)\n",
        "    return packed_tensor, True, None"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BhcDjZz0wyrE"
      },
      "outputs": [],
      "source": [
        "## NOTE - this will be the longest part of the whole process - ensure everything works FIRST before training\n",
        "\n",
        "def train(\n",
        "    dataset, model, tokenizer,\n",
        "    batch_size=16, epochs=5, lr=2e-5,\n",
        "    max_seq_len=400, warmup_steps=200,\n",
        "    gpt2_type=\"gpt2\", output_dir=\".\", output_prefix=\"wreckgar\",\n",
        "    test_mode=False,save_model_on_epoch=False,\n",
        "):\n",
        "    acc_steps = 100\n",
        "    device = torch.device(\"cuda\")\n",
        "    model = model.cuda()\n",
        "    model.train()\n",
        "\n",
        "    optimizer = AdamW(model.parameters(), lr=lr)\n",
        "    scheduler = get_linear_schedule_with_warmup(\n",
        "        optimizer, num_warmup_steps=warmup_steps, num_training_steps=-1\n",
        "    )\n",
        "\n",
        "    train_dataloader = DataLoader(dataset, batch_size=1, shuffle=True)\n",
        "    loss=0\n",
        "    accumulating_batch_count = 0\n",
        "    input_tensor = None\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "\n",
        "        print(f\"Training epoch {epoch}\")\n",
        "        print(loss) ## labels are the tokenized end of the sentence - this is still SL\n",
        "        for idx, entry in tqdm(enumerate(train_dataloader)):\n",
        "            (input_tensor, carry_on, remainder) = pack_tensor(entry, input_tensor, 768)\n",
        "\n",
        "            if carry_on and idx != len(train_dataloader) - 1:\n",
        "                continue\n",
        "\n",
        "            input_tensor = input_tensor.to(device) ## CUDA\n",
        "            outputs = model(input_tensor, labels=input_tensor)\n",
        "            loss = outputs[0]\n",
        "            loss.backward()\n",
        "\n",
        "            if (accumulating_batch_count % batch_size) == 0:\n",
        "                optimizer.step()\n",
        "                scheduler.step()\n",
        "                optimizer.zero_grad()\n",
        "                model.zero_grad()\n",
        "\n",
        "            accumulating_batch_count += 1\n",
        "            input_tensor = None\n",
        "        if save_model_on_epoch:\n",
        "            torch.save(\n",
        "                model.state_dict(),\n",
        "                os.path.join(output_dir, f\"{output_prefix}-{epoch}.pt\"),\n",
        "            )\n",
        "    return model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wYjE9NvzyfuQ"
      },
      "source": [
        "AdamW optimizer is better at generalizing than the Adam optimizer due to how it handles L2 regularization (loss function = loss + sum of squares of weights). Adam is not compatible with L2 since it includes square root of second moment of gradients - this is estunate of uncentered variance of gradients. This allows for an adaptive learning rate (variable gradient descent step size) - high gradients in the past reduce step size (lots of change / very mountainous region) and increases step size for flat regions where we're less likely to converge on a minimum. However, due to its update rule, Adam (unintentionally) regularizes large weights less than small weights. AdamW fixes this by regularizing the weights equally."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model = train(dataset, model, tokenizer)\n",
        "\n",
        "torch.save(model.state_dict(), \"/content/drive/MyDrive/model_large.pth\")"
      ],
      "metadata": {
        "id": "_BGlyiereRFI"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyNLaTAmVsX7RoukJp4b7x3s",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}